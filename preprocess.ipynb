{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('persons.txt', 'r') as f:\n",
    "    text1 = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating test dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, \"'\": 2, '-': 3, '.': 4, 'A': 5, 'B': 6, 'C': 7, 'D': 8, 'E': 9, 'F': 10, 'G': 11, 'H': 12, 'I': 13, 'J': 14, 'K': 15, 'L': 16, 'M': 17, 'N': 18, 'O': 19, 'P': 20, 'Q': 21, 'R': 22, 'S': 23, 'T': 24, 'U': 25, 'V': 26, 'W': 27, 'X': 28, 'Y': 29, 'Z': 30, 'a': 31, 'b': 32, 'c': 33, 'd': 34, 'e': 35, 'f': 36, 'g': 37, 'h': 38, 'i': 39, 'j': 40, 'k': 41, 'l': 42, 'm': 43, 'n': 44, 'o': 45, 'p': 46, 'q': 47, 'r': 48, 's': 49, 't': 50, 'u': 51, 'v': 52, 'w': 53, 'x': 54, 'y': 55, 'z': 56, '<PAD>': 0}\n"
     ]
    }
   ],
   "source": [
    "char2id = dict((c,i) for (i, c) in enumerate(sorted(set(text1.replace('\\n', ' '))),1))\n",
    "id2char = dict(enumerate(sorted(set(text1.replace('\\n', ' '))),1))\n",
    "\n",
    "char2id['<PAD>'] = 0\n",
    "id2char[0] = '<PAD>'\n",
    "\n",
    "print(char2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the data to clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'Terry Bross', 'Alieu Touray-Saidy', 'Albert Prince-Cox', 'Roy Vagelos']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person_names = list(set(text1.split('\\n')))\n",
    "person_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing person data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of raw names  645406\n",
      "Lenght of cleaned person names  641176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Terry Bross',\n",
       " 'Alieu Touray-Saidy',\n",
       " 'Albert Prince-Cox',\n",
       " 'Roy Vagelos',\n",
       " 'Laura de la Torre Tur']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_person_names = []\n",
    "for name in person_names:\n",
    "    if len(name) < 40 and len(name) > 3 and not name.startswith('The '):\n",
    "        cleaned_person_names.append(name)\n",
    "\n",
    "\n",
    "print('Lenght of raw names ', len(person_names))        \n",
    "print('Lenght of cleaned person names ', len(cleaned_person_names))\n",
    "cleaned_person_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[24, 35, 48, 48, 55, 1, 6, 48, 45, 49, 49], [5, 42, 39, 35, 51, 1, 24, 45, 51, 48, 31, 55, 3, 23, 31, 39, 34, 55], [5, 42, 32, 35, 48, 50, 1, 20, 48, 39, 44, 33, 35, 3, 7, 45, 54], [22, 45, 55, 1, 26, 31, 37, 35, 42, 45, 49], [16, 31, 51, 48, 31, 1, 34, 35, 1, 42, 31, 1, 24, 45, 48, 48, 35, 1, 24, 51, 48]]\n"
     ]
    }
   ],
   "source": [
    "nameids = [[char2id[char] for char in name] for name in cleaned_person_names]\n",
    "\n",
    "print(nameids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entities.txt', 'r') as f:\n",
    "    text2 = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Historical-ethnographic museum of Khinalug village',\n",
       " '',\n",
       " 'S.V.VESTA',\n",
       " 'Saint Lucy',\n",
       " 'Mastaangi']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_names = list(set(text2.split('\\n')))\n",
    "entity_names[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and preprocessing entity data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of raw names  1189452\n",
      "Lenght of cleaned person names  1162501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['S.V.VESTA', 'Saint Lucy', 'Mastaangi', 'INS Bimlipatan', 'Tarucus legrasi']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_entity_names = []\n",
    "for name in entity_names:\n",
    "    if len(name) < 40 and len(name) > 3:\n",
    "        cleaned_entity_names.append(name)\n",
    "\n",
    "print('Lenght of raw names ', len(entity_names))        \n",
    "print('Lenght of cleaned person names ', len(cleaned_entity_names))\n",
    "cleaned_entity_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[23, 4, 26, 4, 26, 9, 23, 24, 5], [23, 31, 39, 44, 50, 1, 16, 51, 33, 55], [17, 31, 49, 50, 31, 31, 44, 37, 39], [13, 18, 23, 1, 6, 39, 43, 42, 39, 46, 31, 50, 31, 44], [24, 31, 48, 51, 33, 51, 49, 1, 42, 35, 37, 48, 31, 49, 39]]\n"
     ]
    }
   ],
   "source": [
    "entityids = [[char2id[char] for char in entity] for entity in cleaned_entity_names]\n",
    "print(entityids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "labels = np.concatenate([np.ones((len(nameids),)), np.zeros((len(entityids),))])\n",
    "print(labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1803677"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([24, 35, 48, 48, 55, 1, 6, 48, 45, 49, 49])\n",
      " list([5, 42, 39, 35, 51, 1, 24, 45, 51, 48, 31, 55, 3, 23, 31, 39, 34, 55])\n",
      " list([5, 42, 32, 35, 48, 50, 1, 20, 48, 39, 44, 33, 35, 3, 7, 45, 54])\n",
      " list([22, 45, 55, 1, 26, 31, 37, 35, 42, 45, 49])\n",
      " list([16, 31, 51, 48, 31, 1, 34, 35, 1, 42, 31, 1, 24, 45, 48, 48, 35, 1, 24, 51, 48])]\n"
     ]
    }
   ],
   "source": [
    "inputs = np.concatenate([nameids,entityids])\n",
    "print(inputs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1803677"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs) == len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving inputs and labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_file = open('cleaned_inputs.txt', 'w', encoding='utf-8')\n",
    "labels_file = open('labels.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for text in np.concatenate([cleaned_person_names,cleaned_entity_names]):\n",
    "    inputs_file.write(text+'\\n')\n",
    "    \n",
    "for label in labels:\n",
    "    labels_file.write(str(label)+'\\n')\n",
    "    \n",
    "inputs_file.close()\n",
    "labels_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
